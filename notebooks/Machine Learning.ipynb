{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13a512b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we are going to load some data, preprocess the data so it is in a suitable format for machine learning, and then fit and evaluate a machine learning model. More specifically, we are going to:\n",
    "1. Use the `pandas` package to load our (predictive) Analytics Base Table (ABT) `.csv` data\n",
    "2. Use the `sklearn.preprocessing` package to preprocess the descriptive features in the ABT\n",
    "3. Use the `LogisticRegression` class to train (A.K.A. \"fit\") a logistic regression model\n",
    "4. Use the `sklearn.metrics` package to evaluate the performance of the model\n",
    "\n",
    "Let's begin by loading in the required packages, along with `matplotlib` and `plotnine` for creating some visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f89ccd",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "Let's collect the ABT directly from the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repo_url = \"https://github.com/data-analytics-in-business/gabor-firm-exit-case-study/raw/main/data/\"\n",
    "data_file = \"sample_2012_ABT.csv\"\n",
    "ABT = pd.read_csv(data_repo_url+data_file)\n",
    "ABT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4b2e2",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Before we can pass our data to a machine learning algorithm, we need to preprcoess the data so that it is in a suitable format. How we preprocess the data will vary depending on the type of data.\n",
    "\n",
    "First, let's split out ABT into a target feature `y` and descriptive features `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ABT['default']\n",
    "X = ABT.drop(columns=['default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0f950",
   "metadata": {},
   "source": [
    "Now let's split our descriptive features (`X`) into those that are *numeric* (`X_num`) and those that are *categorical* (`X_cat`), and preprocess them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c205978",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X[['log_sales','p+l_scaled']]\n",
    "X_cat = X['ind_cat'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb903b1f",
   "metadata": {},
   "source": [
    "**Note**: We use the `.reshape(-1, 1)` method to ensure `X_cat` is the correct shape. This is something we need to do when preprocessing a one-dimensional feature array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f5030",
   "metadata": {},
   "source": [
    "## Preprocessing numeric features\n",
    "To preprocess the numeric variables, we are going to use the `MinMaxScaler` [class](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to scale each numeric feature to be in the range `(0,1)`. Let's create an instance of `MinMaxScaler` and then `fit` and `transform` in one step. Finally, we are going to convert `X_num_scaled` back into a `DataFrame` and `describe` it so we can see the preprocessing step has worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "X_num_scaled = minmax_scaler.fit_transform(X_num)\n",
    "X_num_scaled = pd.DataFrame(X_num_scaled, columns=X_num.columns)\n",
    "X_num_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0383901",
   "metadata": {},
   "source": [
    "## Preprocessing categorical features\n",
    "To preprocess the categorical variables, we are going to use the `OneHotEncoder` [class](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to encode each categorical feature as a one-hot numeric array. Let's create an instance of `OneHotEncoder` and then `fit` and `transform` in one step. Finally, we are going to convert `X_cat_onehot` back into a `DataFrame` and print the `head` so we can see the preprocessing step has worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947cbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_encoder = OneHotEncoder(drop='first', handle_unknown=\"ignore\", sparse=False)\n",
    "X_cat_onehot = hot_encoder.fit_transform(X_cat)\n",
    "X_cat_onehot = pd.DataFrame(X_cat_onehot,columns=hot_encoder.get_feature_names_out(['ind_cat']))\n",
    "X_cat_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c9003",
   "metadata": {},
   "source": [
    "## Re-joining numeric and categorical features\n",
    "Before we train our model, we are going to re-join our (preprocessed) numeric and categorical features into one `X_preprocessed` array of our (processed) descriptive features. Let's print the `head` to check things look OK.\n",
    "\n",
    "**Note**: We can concatenate (`concat`) them only because the ordering of our rows/examples have not changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.concat([X_num_scaled, X_cat_onehot], axis=1)\n",
    "X_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84ab94",
   "metadata": {},
   "source": [
    "# Fit model\n",
    "Now we have preprocessed our features to a suitable format, we can use `X_processed` and `y` to train a machine learning model. Let's create an instance of the `LogisticRegression` [class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and `fit` the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_processed,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa0e29",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "Let's now assume that we had this model back in 2012 when the descriptive features were collected, ignoring the fact that we wouldn't have had the data to train the model :)\n",
    "\n",
    "Refreshing our memories of our application domain, we want to use the model to predict the likelihood a company will default, and then use those probabilities to decide whether to approve a loan or not (assuming one was applied for).\n",
    "\n",
    "More precisely, we will:\n",
    "1. Use the model to generate an estimate of the probability that each company will default\n",
    "2. Threshold the probability values to decide whether to approve a loan or not\n",
    "3. Compare these decisions to whether the company defaulted in the future or not\n",
    "\n",
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be660570",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_default = model.predict_proba(X_processed)[:, 1]\n",
    "threshold = 0.5 # Better threshold for loan approval => 0.091\n",
    "loan_approved = prob_default < threshold\n",
    "company_defaults = y\n",
    "\n",
    "ax= plt.subplot()\n",
    "ConfusionMatrixDisplay.from_predictions(company_defaults,loan_approved,ax=ax,display_labels=['no','yes'])\n",
    "ax.set_xlabel('Loan approved')\n",
    "ax.set_ylabel('Company defaults')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881419fd",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "1. Inspect the above visualisation and discuss whether it shows good performance or not.\n",
    "2. Explore different values for `threshold` and justify some different values.\n",
    "3. If you have reached this point, return to the original dataset and consider which additional features would be worth incorporating into the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
